# ProbePosts
Microblogging for UFO hunters.

TODO: longer description

![Screenshot of the ProbePosts home page](screenshot.png)

# How to run
Clone the repository and run the following command to install the required packages (preferably in a venv):
```
$ pip install -r requirements.txt
```
Create a new database in pgAdmin (preferably named ProbePosts) and add the following to an `.env` file in the project root:
```
DB_HOST=localhost
DB_PORT=5432
DB_NAME=ProbePosts
DB_USERNAME=postgres
DB_PASSWORD=postgres
```
If these default values don't match your setup (you might for example have a different password), modify the `.env` as needed.

To fill the database with data, navigate to the `/data/` folder and run the `init_db.py` script:
```
$ py init_db.py
```
This will fill the data from the `/data/tables/` csv files into their corresponding tables in the postgres database specified in the `.env` file. For more on this data was generated, see the 'Generating datasets' section.

The server can then be started with:
```
$ flask run
```
ProbePosts should then be available at https://localhost:5000/

## Generating datasets
The raw UFO sightings dataset we are using for this project can be found in `/data/dataset_raw.csv.old`. The dataset is a scrape of UFO sighting reports from  https://nuforc.org and is publically available on [Kaggle](https://www.kaggle.com/datasets/joebeachcapital/ufo-sightings/data). We have cleaned this dataset up using the `/data/remove_videos.py` script, which filters out reports with non-image attachments or links that 404. This script has been used to generate the `dataset_raw.csv` data we are using for this project.

To generate all the tables used by ProbePosts from this dataset, you can navigate to `/data/` and run
```
$ py generate_dataset.py
```
This will generate csv files for posts, users, follow relationships, etc in the `/data/tables/` folder.

Note that the image urls for posts are not included in the generated posts.csv table. These are instead stored seperately in `/data/image_urls.csv`, which can be generated by running the `scrape_image_urls.py` script, which will scrape the images from the reports listed in `dataset_raw.csv`. Please not that this script takes a *long* time to run, since the UFO sighting reports website is *very* slow. It is not recommended that you run this yourself.

The `init_db.py` script will then combine these datasets to give the posts their proper image urls. If an image url for a post is missing in the `image_urls.csv` file (or if the file is empty), posts are just assigned a dummy image from https://picsum.photos/512.
